{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnWbS1sUNQN"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as K \n",
        "\n",
        "import util"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "WCwMymd4UPwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sub-volumes\n",
        "def get_sub_volume(image, label, \n",
        "                   orig_x = 240, orig_y = 240, orig_z = 155, \n",
        "                   output_x = 160, output_y = 160, output_z = 16,\n",
        "                   num_classes = 4, max_tries = 1000, \n",
        "                   background_threshold=0.95):\n",
        "    \"\"\"\n",
        "    Extract random sub-volume from original images.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): original image, \n",
        "            of shape (orig_x, orig_y, orig_z, num_channels)\n",
        "        label (np.array): original label. \n",
        "            labels coded using discrete values rather than\n",
        "            a separate dimension, \n",
        "            so this is of shape (orig_x, orig_y, orig_z)\n",
        "        orig_x (int): x_dim of input image\n",
        "        orig_y (int): y_dim of input image\n",
        "        orig_z (int): z_dim of input image\n",
        "        output_x (int): desired x_dim of output\n",
        "        output_y (int): desired y_dim of output\n",
        "        output_z (int): desired z_dim of output\n",
        "        num_classes (int): number of class labels\n",
        "        max_tries (int): maximum trials to do when sampling\n",
        "        background_threshold (float): limit on the fraction \n",
        "            of the sample which can be the background\n",
        "\n",
        "    returns:\n",
        "        X (np.array): sample of original image of dimension \n",
        "            (num_channels, output_x, output_y, output_z)\n",
        "        y (np.array): labels which correspond to X, of dimension \n",
        "            (num_classes, output_x, output_y, output_z)\n",
        "    \"\"\"\n",
        "    # Initialize features and labels with `None`\n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    tries = 0\n",
        "    \n",
        "    while tries < max_tries:\n",
        "        # randomly sample sub-volume by sampling the corner voxel\n",
        "        # hint: make sure to leave enough room for the output dimensions!\n",
        "        start_x = np.random.randint(0, orig_x - output_x+1)\n",
        "        start_y = np.random.randint(0, orig_y - output_y+1)\n",
        "        start_z = np.random.randint(0, orig_z - output_z+1)\n",
        "\n",
        "        # extract relevant area of label\n",
        "        y = label[start_x: start_x + output_x,\n",
        "                  start_y: start_y + output_y,\n",
        "                  start_z: start_z + output_z]\n",
        "        \n",
        "        # One-hot encode the categories.\n",
        "        # This adds a 4th dimension, 'num_classes'\n",
        "        # (output_x, output_y, output_z, num_classes)\n",
        "        y = keras.utils.to_categorical(y, num_classes=num_classes)\n",
        "\n",
        "        # compute the background ratio\n",
        "        bgrd_ratio = np.sum(y[:, :, :, 0])/(output_x * output_y * output_z)\n",
        "\n",
        "        # increment tries counter\n",
        "        tries += 1\n",
        "\n",
        "        # if background ratio is below the desired threshold,\n",
        "        # use that sub-volume.\n",
        "        # otherwise continue the loop and try another random sub-volume\n",
        "        if bgrd_ratio < background_threshold:\n",
        "\n",
        "            # make copy of the sub-volume\n",
        "            X = np.copy(image[start_x: start_x + output_x,\n",
        "                              start_y: start_y + output_y,\n",
        "                              start_z: start_z + output_z, :])\n",
        "            \n",
        "            # change dimension of X\n",
        "            # from (x_dim, y_dim, z_dim, num_channels)\n",
        "            # to (num_channels, x_dim, y_dim, z_dim)\n",
        "            X = np.moveaxis(X, 3, 0)\n",
        "\n",
        "            # change dimension of y\n",
        "            # from (x_dim, y_dim, z_dim, num_classes)\n",
        "            # to (num_classes, x_dim, y_dim, z_dim)\n",
        "            y = np.moveaxis(y, 3, 0)\n",
        "            \n",
        "            # take a subset of y that excludes the background class\n",
        "            # in the 'num_classes' dimension\n",
        "            y = y[1:, :, :, :]\n",
        "    \n",
        "            return X, y\n",
        "\n",
        "    # if we've tried max_tries number of samples\n",
        "    # Give up in order to avoid looping forever.\n",
        "    print(f\"Tried {tries} times to find a sub-volume. Giving up...\")"
      ],
      "metadata": {
        "id": "rp2PUl4kUP32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_001.nii.gz\", DATA_DIR + \"labelsTr/BRATS_001.nii.gz\")\n",
        "X, y = get_sub_volume(image, label)\n",
        "# enhancing tumor is channel 2 in the class label\n",
        "# you can change indexer for y to look at different classes\n",
        "util.visualize_patch(X[0, :, :, :], y[2])"
      ],
      "metadata": {
        "id": "XlKkw2e9UP61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization"
      ],
      "metadata": {
        "id": "E3nXEIuoVCHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(image):\n",
        "    \"\"\"\n",
        "    Standardize mean and standard deviation \n",
        "        of each channel and z_dimension.\n",
        "\n",
        "    Args:\n",
        "        image (np.array): input image, \n",
        "            shape (num_channels, dim_x, dim_y, dim_z)\n",
        "\n",
        "    Returns:\n",
        "        standardized_image (np.array): standardized version of input image\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize to array of zeros, with same shape as the image\n",
        "    standardized_image = np.zeros(image.shape)\n",
        "\n",
        "    # iterate over channels\n",
        "    for c in range(image.shape[0]):\n",
        "        # iterate over the `z` dimension\n",
        "        for z in range(image.shape[3]):\n",
        "            # get a slice of the image \n",
        "            # at channel c and z-th dimension `z`\n",
        "            image_slice = image[c,:,:,z]\n",
        "\n",
        "            # subtract the mean from image_slice\n",
        "            centered = image_slice - np.mean(image_slice)\n",
        "            \n",
        "            # divide by the standard deviation (only if it is different from zero)\n",
        "            if np.std(centered) != 0:\n",
        "                centered_scaled = centered / np.std(centered)\n",
        "\n",
        "                # update  the slice of standardized image\n",
        "                # with the scaled centered and scaled image\n",
        "                standardized_image[c, :, :, z] = centered_scaled\n",
        "\n",
        "    return standardized_image"
      ],
      "metadata": {
        "id": "PoBk7DPaUQAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm = standardize(X)\n",
        "print(\"standard deviation for a slice should be 1.0\")\n",
        "print(f\"stddv for X_norm[0, :, :, 0]: {X_norm[0,:,:,0].std():.2f}\")"
      ],
      "metadata": {
        "id": "TJYj0x71UQCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.visualize_patch(X_norm[0, :, :, :], y[2])"
      ],
      "metadata": {
        "id": "E1GxMlc6UQF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model: 3D U-Net"
      ],
      "metadata": {
        "id": "RRRCn3DQVOqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xVg3LJw_UQIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oIRXKHheUQK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "vi_adDZPVT1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dice Similarity Coefficient"
      ],
      "metadata": {
        "id": "ZoZll2CIVVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_class_dice_coefficient(y_true, y_pred, axis=(0, 1, 2), \n",
        "                                  epsilon=0.00001):\n",
        "    \"\"\"\n",
        "    Compute dice coefficient for single class.\n",
        "\n",
        "    Args:\n",
        "        y_true (Tensorflow tensor): tensor of ground truth values for single class.\n",
        "                                    shape: (x_dim, y_dim, z_dim)\n",
        "        y_pred (Tensorflow tensor): tensor of predictions for single class.\n",
        "                                    shape: (x_dim, y_dim, z_dim)\n",
        "        axis (tuple): spatial axes to sum over when computing numerator and\n",
        "                      denominator of dice coefficient.\n",
        "                      Hint: pass this as the 'axis' argument to the K.sum function.\n",
        "        epsilon (float): small constant added to numerator and denominator to\n",
        "                        avoid divide by 0 errors.\n",
        "    Returns:\n",
        "        dice_coefficient (float): computed value of dice coefficient.     \n",
        "    \"\"\"\n",
        "\n",
        "    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n",
        "    dice_denominator = K.sum(y_true, axis=axis) + K.sum(y_pred, axis=axis) + epsilon\n",
        "    dice_coefficient = (dice_numerator) / (dice_denominator)\n",
        "\n",
        "\n",
        "    return dice_coefficient"
      ],
      "metadata": {
        "id": "IECkVhTnUQNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CASES\n",
        "sess = K.get_session()\n",
        "#sess = tf.compat.v1.Session()\n",
        "with sess.as_default() as sess:\n",
        "    pred = np.expand_dims(np.eye(2), -1)\n",
        "    label = np.expand_dims(np.array([[1.0, 1.0], [0.0, 0.0]]), -1)\n",
        "\n",
        "    print(\"Test Case #1\")\n",
        "    print(\"pred:\")\n",
        "    print(pred[:, :, 0])\n",
        "    print(\"label:\")\n",
        "    print(label[:, :, 0])\n",
        "\n",
        "    # choosing a large epsilon to help check for implementation errors\n",
        "    dc = single_class_dice_coefficient(pred, label,epsilon=1)\n",
        "    print(f\"dice coefficient: {dc.eval():.4f}\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Test Case #2\")\n",
        "    pred = np.expand_dims(np.eye(2), -1)\n",
        "    label = np.expand_dims(np.array([[1.0, 1.0], [0.0, 1.0]]), -1)\n",
        "\n",
        "    print(\"pred:\")\n",
        "    print(pred[:, :, 0])\n",
        "    print(\"label:\")\n",
        "    print(label[:, :, 0])\n",
        "\n",
        "    # choosing a large epsilon to help check for implementation errors\n",
        "    dc = single_class_dice_coefficient(pred, label,epsilon=1)\n",
        "    print(f\"dice_coefficient: {dc.eval():.4f}\")"
      ],
      "metadata": {
        "id": "yyE_FWAgVcY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dice Coefficient for Multiple classes"
      ],
      "metadata": {
        "id": "5bu9eqfSViag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(y_true, y_pred, axis=(1, 2, 3), \n",
        "                     epsilon=0.00001):\n",
        "    \"\"\"\n",
        "    Compute mean dice coefficient over all abnormality classes.\n",
        "\n",
        "    Args:\n",
        "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
        "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
        "        y_pred (Tensorflow tensor): tensor of predictions for all classes.\n",
        "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
        "        axis (tuple): spatial axes to sum over when computing numerator and\n",
        "                      denominator of dice coefficient.\n",
        "                      Hint: pass this as the 'axis' argument to the K.sum\n",
        "                            and K.mean functions.\n",
        "        epsilon (float): small constant add to numerator and denominator to\n",
        "                        avoid divide by 0 errors.\n",
        "    Returns:\n",
        "        dice_coefficient (float): computed value of dice coefficient.     \n",
        "    \"\"\"\n",
        "  \n",
        "    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n",
        "    dice_denominator = K.sum(y_true, axis=axis) + K.sum(y_pred, axis=axis) + epsilon\n",
        "    dice_coefficient = K.mean((dice_numerator)/(dice_denominator))\n",
        "\n",
        "    return dice_coefficient"
      ],
      "metadata": {
        "id": "Q5lQd4uiVccC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Soft Dice Loss"
      ],
      "metadata": {
        "id": "3_Ck6RJXVtol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_dice_loss(y_true, y_pred, axis=(1, 2, 3), \n",
        "                   epsilon=0.00001):\n",
        "    \"\"\"\n",
        "    Compute mean soft dice loss over all abnormality classes.\n",
        "\n",
        "    Args:\n",
        "        y_true (Tensorflow tensor): tensor of ground truth values for all classes.\n",
        "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
        "        y_pred (Tensorflow tensor): tensor of soft predictions for all classes.\n",
        "                                    shape: (num_classes, x_dim, y_dim, z_dim)\n",
        "        axis (tuple): spatial axes to sum over when computing numerator and\n",
        "                      denominator in formula for dice loss.\n",
        "                      Hint: pass this as the 'axis' argument to the K.sum\n",
        "                            and K.mean functions.\n",
        "        epsilon (float): small constant added to numerator and denominator to\n",
        "                        avoid divide by 0 errors.\n",
        "    Returns:\n",
        "        dice_loss (float): computed value of dice loss.     \n",
        "    \"\"\"\n",
        "\n",
        "    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n",
        "    dice_denominator = K.sum(y_true**2, axis=axis) + K.sum(y_pred**2, axis=axis) + epsilon\n",
        "    dice_loss = 1 - K.mean((dice_numerator)/(dice_denominator))\n",
        "\n",
        "    return dice_loss"
      ],
      "metadata": {
        "id": "jrsXRtBDVcfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Train the model"
      ],
      "metadata": {
        "id": "X-vz8FHyV40q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient])"
      ],
      "metadata": {
        "id": "a_1q34VrVcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training on a Large Dataset"
      ],
      "metadata": {
        "id": "TC4YGW6HV-b-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# May cause the kernel to die if running in the Coursera platform\n",
        "\n",
        "base_dir = HOME_DIR + \"processed/\"\n",
        "\n",
        "with open(base_dir + \"config.json\") as json_file:\n",
        "    config = json.load(json_file)\n",
        "\n",
        "# Get generators for training and validation sets\n",
        "train_generator = util.VolumeDataGenerator(config[\"train\"], base_dir + \"train/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
        "valid_generator = util.VolumeDataGenerator(config[\"valid\"], base_dir + \"valid/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
        "\n",
        "steps_per_epoch = 20\n",
        "n_epochs=10\n",
        "validation_steps = 20\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=n_epochs,\n",
        "        use_multiprocessing=True,\n",
        "        validation_data=valid_generator,\n",
        "        validation_steps=validation_steps)\n",
        "\n",
        "# run this cell if you to save the weights of your trained model in cell section 4.1\n",
        "#model.save_weights(base_dir + 'my_model_pretrained.hdf5')"
      ],
      "metadata": {
        "id": "NItaqwBcV6wL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading a Pre-Trained Model"
      ],
      "metadata": {
        "id": "q2aVuaupWHDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell if you didn't run the training cell in section 4.1\n",
        "base_dir = HOME_DIR + \"processed/\"\n",
        "with open(base_dir + \"config.json\") as json_file:\n",
        "    config = json.load(json_file)\n",
        "# Get generators for training and validation sets\n",
        "train_generator = util.VolumeDataGenerator(config[\"train\"], base_dir + \"train/\", batch_size=3, dim=(160, 160, 16), verbose=0)\n",
        "valid_generator = util.VolumeDataGenerator(config[\"valid\"], base_dir + \"valid/\", batch_size=3, dim=(160, 160, 16), verbose=0)"
      ],
      "metadata": {
        "id": "ZybrocJ0WIa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(HOME_DIR + \"model_pretrained.hdf5\")"
      ],
      "metadata": {
        "id": "hEqeJOL3V6zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "B5OBKQAsUQQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "FzemCkwsWPHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall Performance"
      ],
      "metadata": {
        "id": "k9RqvdQfWQBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=0)\n",
        "patch_pred = model.predict(X_norm_with_batch_dimension)"
      ],
      "metadata": {
        "id": "6rEz1vWPWW2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set threshold.\n",
        "threshold = 0.5\n",
        "\n",
        "# use threshold to get hard predictions\n",
        "patch_pred[patch_pred > threshold] = 1.0\n",
        "patch_pred[patch_pred <= threshold] = 0.0"
      ],
      "metadata": {
        "id": "WXn7Uz2-WW5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Patch and ground truth\")\n",
        "util.visualize_patch(X_norm[0, :, :, :], y[2])\n",
        "plt.show()\n",
        "print(\"Patch and prediction\")\n",
        "util.visualize_patch(X_norm[0, :, :, :], patch_pred[0, 2, :, :, :])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zeDC2R28WZUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sensitivity and Specificity"
      ],
      "metadata": {
        "id": "S6ybmX_tWdmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_sens_spec(pred, label, class_num):\n",
        "    \"\"\"\n",
        "    Compute sensitivity and specificity for a particular example\n",
        "    for a given class.\n",
        "\n",
        "    Args:\n",
        "        pred (np.array): binary arrary of predictions, shape is\n",
        "                         (num classes, height, width, depth).\n",
        "        label (np.array): binary array of labels, shape is\n",
        "                          (num classes, height, width, depth).\n",
        "        class_num (int): number between 0 - (num_classes -1) which says\n",
        "                         which prediction class to compute statistics\n",
        "                         for.\n",
        "\n",
        "    Returns:\n",
        "        sensitivity (float): precision for given class_num.\n",
        "        specificity (float): recall for given class_num\n",
        "    \"\"\"\n",
        "\n",
        "    # extract sub-array for specified class\n",
        "    class_pred = pred[class_num]\n",
        "    class_label = label[class_num]\n",
        "\n",
        "    # true positives\n",
        "    tp = np.sum((class_pred == 1) & (class_label == 1))\n",
        "\n",
        "    # true negatives\n",
        "    tn = np.sum((class_pred == 0) & (class_label == 0))\n",
        "    \n",
        "    #false positives\n",
        "    fp = np.sum((class_pred == 1) & (class_label == 0))\n",
        "    \n",
        "    # false negatives\n",
        "    fn = np.sum((class_pred == 0) & (class_label == 1))\n",
        "\n",
        "    # compute sensitivity and specificity\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "\n",
        "    return sensitivity, specificity"
      ],
      "metadata": {
        "id": "vEJ603F8WZXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we must explicity import 'display' in order for the autograder to compile the submitted code\n",
        "# Even though we could use this function without importing it, keep this import in order to allow the grader to work\n",
        "from IPython.display import display\n",
        "print(\"Test Case #3\")\n",
        "\n",
        "df = pd.DataFrame({'y_test': [1,1,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "                   'preds_test': [1,1,0,0,0,1,1,1,1,0,0,0,0,0],\n",
        "                   'category': ['TP','TP','TN','TN','TN','FP','FP','FP','FP','FN','FN','FN','FN','FN']\n",
        "                  })\n",
        "\n",
        "display(df)\n",
        "pred = np.array( [df['preds_test']])\n",
        "label = np.array( [df['y_test']])\n",
        "\n",
        "sensitivity, specificity = compute_class_sens_spec(pred, label, 0)\n",
        "print(f\"sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "24Y5ug0AWZaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we must explicity import 'display' in order for the autograder to compile the submitted code\n",
        "# Even though we could use this function without importing it, keep this import in order to allow the grader to work\n",
        "from IPython.display import display\n",
        "print(\"Test Case #3\")\n",
        "\n",
        "df = pd.DataFrame({'y_test': [1,1,0,0,0,0,0,0,0,1,1,1,1,1],\n",
        "                   'preds_test': [1,1,0,0,0,1,1,1,1,0,0,0,0,0],\n",
        "                   'category': ['TP','TP','TN','TN','TN','FP','FP','FP','FP','FN','FN','FN','FN','FN']\n",
        "                  })\n",
        "\n",
        "display(df)\n",
        "pred = np.array( [df['preds_test']])\n",
        "label = np.array( [df['y_test']])\n",
        "\n",
        "sensitivity, specificity = compute_class_sens_spec(pred, label, 0)\n",
        "print(f\"sensitivity: {sensitivity:.4f}\")\n",
        "print(f\"specificity: {specificity:.4f}\")"
      ],
      "metadata": {
        "id": "JELNDWhLWW8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sens_spec_df(pred, label):\n",
        "    patch_metrics = pd.DataFrame(\n",
        "        columns = ['Edema', \n",
        "                   'Non-Enhancing Tumor', \n",
        "                   'Enhancing Tumor'], \n",
        "        index = ['Sensitivity',\n",
        "                 'Specificity'])\n",
        "    \n",
        "    for i, class_name in enumerate(patch_metrics.columns):\n",
        "        sens, spec = compute_class_sens_spec(pred, label, i)\n",
        "        patch_metrics.loc['Sensitivity', class_name] = round(sens,4)\n",
        "        patch_metrics.loc['Specificity', class_name] = round(spec,4)\n",
        "\n",
        "    return patch_metrics"
      ],
      "metadata": {
        "id": "qSb5ZUYdWqLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_sens_spec_df(patch_pred[0], y)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ddVSkBxqWtt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running on entire scans"
      ],
      "metadata": {
        "id": "8CK87dsEWxS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment this code to run it\n",
        "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_001.nii.gz\", DATA_DIR + \"labelsTr/BRATS_001.nii.gz\")\n",
        "pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77))  "
      ],
      "metadata": {
        "id": "NLTARJDaWqOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = load_case(DATA_DIR + \"imagesTr/BRATS_003.nii.gz\", DATA_DIR + \"labelsTr/BRATS_003.nii.gz\")\n",
        "pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77)) "
      ],
      "metadata": {
        "id": "whE9SVeAW56h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_scan_label = keras.utils.to_categorical(label, num_classes = 4)\n",
        "whole_scan_pred = pred\n",
        "\n",
        "# move axis to match shape expected in functions\n",
        "whole_scan_label = np.moveaxis(whole_scan_label, 3 ,0)[1:4]\n",
        "whole_scan_pred = np.moveaxis(whole_scan_pred, 3, 0)[1:4]"
      ],
      "metadata": {
        "id": "rJRKha-sW593"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label)\n",
        "\n",
        "print(whole_scan_df)"
      ],
      "metadata": {
        "id": "KGlNxh2YW6A2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINISH"
      ],
      "metadata": {
        "id": "ael5NgaCW_m_"
      }
    }
  ]
}