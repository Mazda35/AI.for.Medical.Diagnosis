{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluation of Diagnostic Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfWQKOXBXIor"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd  \n",
        "\n",
        "import util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results = pd.read_csv(\"train_preds.csv\")\n",
        "valid_results = pd.read_csv(\"valid_preds.csv\")\n",
        "\n",
        "# the labels in our dataset\n",
        "class_labels = ['Cardiomegaly',\n",
        " 'Emphysema',\n",
        " 'Effusion',\n",
        " 'Hernia',\n",
        " 'Infiltration',\n",
        " 'Mass',\n",
        " 'Nodule',\n",
        " 'Atelectasis',\n",
        " 'Pneumothorax',\n",
        " 'Pleural_Thickening',\n",
        " 'Pneumonia',\n",
        " 'Fibrosis',\n",
        " 'Edema',\n",
        " 'Consolidation']\n",
        "\n",
        "# the labels for prediction values in our dataset\n",
        "pred_labels = [l + \"_pred\" for l in class_labels]"
      ],
      "metadata": {
        "id": "uZENqW0-XKvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = valid_results[class_labels].values\n",
        "pred = valid_results[pred_labels].values"
      ],
      "metadata": {
        "id": "972F8b-ZXKxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_results[np.concatenate([class_labels, pred_labels])].head()"
      ],
      "metadata": {
        "id": "zPaSW74WXK2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xticks(rotation=90)\n",
        "plt.bar(x = class_labels, height= y.sum(axis=0))"
      ],
      "metadata": {
        "id": "6paVvendXK5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "gj7ImahpXKWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def true_positives(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Count true positives.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        TP (int): true positives\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    \n",
        "    # get thresholded predictions\n",
        "    thresholded_preds = pred >= th\n",
        "\n",
        "    # compute TP\n",
        "    TP = np.sum((y == 1) & (thresholded_preds == 1))\n",
        "    \n",
        "    return TP\n",
        "\n",
        "def true_negatives(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Count true negatives.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        TN (int): true negatives\n",
        "    \"\"\"\n",
        "    TN = 0\n",
        "    \n",
        "    # get thresholded predictions\n",
        "    thresholded_preds = pred >= th\n",
        "\n",
        "    # compute TN\n",
        "    TN = np.sum((y == 0) & (thresholded_preds == 0))\n",
        "\n",
        "    return TN\n",
        "\n",
        "def false_positives(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Count false positives.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        FP (int): false positives\n",
        "    \"\"\"\n",
        "    FP = 0\n",
        "    \n",
        "    # get thresholded predictions\n",
        "    thresholded_preds = pred >= th\n",
        "\n",
        "    # compute FP\n",
        "    FP = np.sum((y == 0) & (thresholded_preds == 1))\n",
        "    \n",
        "    return FP\n",
        "\n",
        "def false_negatives(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Count false positives.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        FN (int): false negatives\n",
        "    \"\"\"\n",
        "    FN = 0\n",
        "    \n",
        "    # get thresholded predictions\n",
        "    thresholded_preds = pred >= th\n",
        "\n",
        "    # compute FN\n",
        "    FN = np.sum((y == 1) & (thresholded_preds == 0))\n",
        "\n",
        "    return FN\n",
        "\n",
        "def get_accuracy(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Compute accuracy of predictions at threshold.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        accuracy (float): accuracy of predictions at threshold\n",
        "    \"\"\"\n",
        "    accuracy = 0.0\n",
        "\n",
        "    # get TP, FP, TN, FN using our previously defined functions\n",
        "    TP = true_positives(y, pred, th=th)\n",
        "    FP = false_positives(y, pred, th=th)\n",
        "    TN = true_negatives(y, pred, th=th)\n",
        "    FN = false_negatives(y, pred, th=th)\n",
        "\n",
        "    # Compute accuracy using TP, FP, TN, FN\n",
        "    accuracy = (TP+TN)/(TP+FP+TN+FN)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def get_prevalence(y):\n",
        "    \"\"\"\n",
        "    Compute prevalence.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "    Returns:\n",
        "        prevalence (float): prevalence of positive cases\n",
        "    \"\"\"\n",
        "    prevalence = 0.0\n",
        " \n",
        "    prevalence = np.mean((y==1))\n",
        "\n",
        "    return prevalence"
      ],
      "metadata": {
        "id": "_8vVRsPdXK8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_accuracy(valid_results[\"Emphysema\"].values, np.zeros(len(valid_results)))"
      ],
      "metadata": {
        "id": "zIjPbu9sXK_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sensitivity(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Compute sensitivity of predictions at threshold.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n",
        "    \"\"\"\n",
        "    sensitivity = 0.0\n",
        "\n",
        "    # get TP and FN using our previously defined functions\n",
        "    TP = true_positives(y,pred,th=th)\n",
        "    FN = false_negatives(y,pred,th=th)\n",
        "\n",
        "    # use TP and FN to compute sensitivity\n",
        "    sensitivity = TP/(TP+FN)\n",
        "\n",
        "    return sensitivity\n",
        "\n",
        "def get_specificity(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Compute specificity of predictions at threshold.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        specificity (float): probability that the test outputs negative given that the case is actually negative\n",
        "    \"\"\"\n",
        "    specificity = 0.0\n",
        "\n",
        "    # get TN and FP using our previously defined functions\n",
        "    TN = true_negatives(y,pred,th=th)\n",
        "    FP = false_positives(y,pred,th=th)\n",
        "    \n",
        "    # use TN and FP to compute specificity \n",
        "    specificity = TN/(TN+FP)\n",
        "\n",
        "    return specificity"
      ],
      "metadata": {
        "id": "BTQfEm_zXLB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PPV and NPV"
      ],
      "metadata": {
        "id": "fl6AY6qaalOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ppv(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Compute PPV of predictions at threshold.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        PPV (float): positive predictive value of predictions at threshold\n",
        "    \"\"\"\n",
        "    PPV = 0.0\n",
        "\n",
        "    # get TP and FP using our previously defined functions\n",
        "    TP = true_positives(y,pred,th=th)\n",
        "    FP = false_positives(y,pred,th=th)\n",
        "\n",
        "    # use TP and FP to compute PPV\n",
        "    PPV = TP/(TP+FP)\n",
        "\n",
        "    return PPV\n",
        "\n",
        "def get_npv(y, pred, th=0.5):\n",
        "    \"\"\"\n",
        "    Compute NPV of predictions at threshold.\n",
        "\n",
        "    Args:\n",
        "        y (np.array): ground truth, size (n_examples)\n",
        "        pred (np.array): model output, size (n_examples)\n",
        "        th (float): cutoff value for positive prediction from model\n",
        "    Returns:\n",
        "        NPV (float): negative predictive value of predictions at threshold\n",
        "    \"\"\"\n",
        "    NPV = 0.0\n",
        "\n",
        "    # get TN and FN using our previously defined functions\n",
        "    TN = true_negatives(y,pred,th=th)\n",
        "    FN = false_negatives(y,pred,th=th)\n",
        "\n",
        "    # use TN and FN to compute NPV\n",
        "    NPV = TN/(TN+FN)\n",
        "\n",
        "    return NPV"
      ],
      "metadata": {
        "id": "1yw_vPBOXLE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Intervals"
      ],
      "metadata": {
        "id": "oQka1Ywoa4y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n",
        "    statistics = np.zeros((len(classes), bootstraps))\n",
        "\n",
        "    for c in range(len(classes)):\n",
        "        df = pd.DataFrame(columns=['y', 'pred'])\n",
        "        df.loc[:, 'y'] = y[:, c]\n",
        "        df.loc[:, 'pred'] = pred[:, c]\n",
        "        # get positive examples for stratified sampling\n",
        "        df_pos = df[df.y == 1]\n",
        "        df_neg = df[df.y == 0]\n",
        "        prevalence = len(df_pos) / len(df)\n",
        "        for i in range(bootstraps):\n",
        "            # stratified sampling of positive and negative examples\n",
        "            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n",
        "            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n",
        "\n",
        "            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n",
        "            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n",
        "            score = roc_auc_score(y_sample, pred_sample)\n",
        "            statistics[c][i] = score\n",
        "    return statistics\n",
        "\n",
        "statistics = bootstrap_auc(y, pred, class_labels)\n",
        "\n",
        "util.print_confidence_intervals(class_labels, statistics)"
      ],
      "metadata": {
        "id": "0eBBOPOtar8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precision-Recall Curve"
      ],
      "metadata": {
        "id": "CbugyKlga1hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "util.get_curve(y, pred, class_labels, curve='prc')"
      ],
      "metadata": {
        "id": "y8WGnwYHar_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F1 Score"
      ],
      "metadata": {
        "id": "sheJR2DRbLRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n",
        "                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)"
      ],
      "metadata": {
        "id": "qcZf2QnQbQGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibration"
      ],
      "metadata": {
        "id": "ftigrl_RbWhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import calibration_curve\n",
        "def plot_calibration_curve(y, pred):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(class_labels)):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20)\n",
        "        plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "        plt.plot(mean_predicted_value, fraction_of_positives, marker='.')\n",
        "        plt.xlabel(\"Predicted Value\")\n",
        "        plt.ylabel(\"Fraction of Positives\")\n",
        "        plt.title(class_labels[i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "plot_calibration_curve(y, pred)"
      ],
      "metadata": {
        "id": "sZC_jAmXasCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR \n",
        "\n",
        "y_train = train_results[class_labels].values\n",
        "pred_train = train_results[pred_labels].values\n",
        "pred_calibrated = np.zeros_like(pred)\n",
        "\n",
        "for i in range(len(class_labels)):\n",
        "    lr = LR(solver='liblinear', max_iter=10000)\n",
        "    lr.fit(pred_train[:, i].reshape(-1, 1), y_train[:, i])    \n",
        "    pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(-1, 1))[:,1]\n",
        "\n",
        "    \n",
        "plot_calibration_curve(y[:,], pred_calibrated)"
      ],
      "metadata": {
        "id": "73PcnJl4bT87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINISH"
      ],
      "metadata": {
        "id": "91XzZGuUbl1j"
      }
    }
  ]
}