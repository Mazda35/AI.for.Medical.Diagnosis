{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chest X-Ray Medical Diagnosis with Deep Learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chest X-Ray Medical Diagnosis with Deep Learning"
      ],
      "metadata": {
        "id": "TYEu3vBXQ4mo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0huhTFaQvMc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "import util"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Datasets"
      ],
      "metadata": {
        "id": "DxtTiFLKRACr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"nih/train-small.csv\")\n",
        "valid_df = pd.read_csv(\"nih/valid-small.csv\")\n",
        "\n",
        "test_df = pd.read_csv(\"nih/test.csv\")\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "rye_k0NiQ5jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"nih/train-small.csv\")\n",
        "valid_df = pd.read_csv(\"nih/valid-small.csv\")\n",
        "\n",
        "test_df = pd.read_csv(\"nih/test.csv\")\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "mjNcGdJTQ5mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preventing Data Leakage"
      ],
      "metadata": {
        "id": "9LiNJPAFRdDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_leakage(df1, df2, patient_col):\n",
        "    \n",
        "    df1_patients_unique = set(df1[patient_col].values)\n",
        "    df2_patients_unique = set(df2[patient_col].values)\n",
        "    \n",
        "    patients_in_both_groups = list(df1_patients_unique.intersection(df2_patients_unique))\n",
        "\n",
        "    # leakage contains true if there is patient overlap, otherwise false.\n",
        "    if len(patients_in_both_groups)>0:\n",
        "        leakage = True \n",
        "    else:\n",
        "        leakage = False\n",
        "    # boolean (true if there is at least 1 patient in both groups)\n",
        "    \n",
        "    return leakage"
      ],
      "metadata": {
        "id": "8FadNRqJQ5pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test case 1\")\n",
        "df1 = pd.DataFrame({'patient_id': [0, 1, 2]})\n",
        "df2 = pd.DataFrame({'patient_id': [2, 3, 4]})\n",
        "print(\"df1\")\n",
        "print(df1)\n",
        "print(\"df2\")\n",
        "print(df2)\n",
        "print(f\"leakage output: {check_for_leakage(df1, df2, 'patient_id')}\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"test case 2\")\n",
        "df1 = pd.DataFrame({'patient_id': [0, 1, 2]})\n",
        "df2 = pd.DataFrame({'patient_id': [3, 4, 5]})\n",
        "print(\"df1:\")\n",
        "print(df1)\n",
        "print(\"df2:\")\n",
        "print(df2)\n",
        "\n",
        "print(f\"leakage output: {check_for_leakage(df1, df2, 'patient_id')}\")"
      ],
      "metadata": {
        "id": "GO4K7G-iQ5st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"leakage between train and valid: {}\".format(check_for_leakage(train_df, valid_df, 'PatientId')))\n",
        "print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'PatientId')))\n",
        "print(\"leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'PatientId')))"
      ],
      "metadata": {
        "id": "A2TD9t7xQ57t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Images"
      ],
      "metadata": {
        "id": "zM7PWWV2Rjb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
        "\n",
        "    # normalize images\n",
        "    image_generator = ImageDataGenerator(\n",
        "        samplewise_center=True,\n",
        "        samplewise_std_normalization= True)\n",
        "    \n",
        "    # flow from directory with specified batch size\n",
        "    # and target image size\n",
        "    generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    \n",
        "    return generator"
      ],
      "metadata": {
        "id": "Y2-x_4VVRW56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
        "\n",
        "    # get generator to sample dataset\n",
        "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
        "        dataframe=train_df, \n",
        "        directory=IMAGE_DIR, \n",
        "        x_col=\"Image\", \n",
        "        y_col=labels, \n",
        "        class_mode=\"raw\", \n",
        "        batch_size=sample_size, \n",
        "        shuffle=True, \n",
        "        target_size=(target_w, target_h))\n",
        "    \n",
        "    # get data sample\n",
        "    batch = raw_train_generator.next()\n",
        "    data_sample = batch[0]\n",
        "\n",
        "    # use sample to fit mean and std for test set generator\n",
        "    image_generator = ImageDataGenerator(\n",
        "        featurewise_center=True,\n",
        "        featurewise_std_normalization= True)\n",
        "    \n",
        "    # fit generator to sample from training data\n",
        "    image_generator.fit(data_sample)\n",
        "\n",
        "    # get test generator\n",
        "    valid_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=valid_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "\n",
        "    test_generator = image_generator.flow_from_dataframe(\n",
        "            dataframe=test_df,\n",
        "            directory=image_dir,\n",
        "            x_col=x_col,\n",
        "            y_col=y_cols,\n",
        "            class_mode=\"raw\",\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "            target_size=(target_w,target_h))\n",
        "    return valid_generator, test_generator"
      ],
      "metadata": {
        "id": "QOY_qT1LRW8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_DIR = \"nih/images-small/\"\n",
        "train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\", labels)\n",
        "valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\", labels)"
      ],
      "metadata": {
        "id": "5e-5LBOSRW_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development"
      ],
      "metadata": {
        "id": "91x10_mvSCyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Addressing Class Imbalance"
      ],
      "metadata": {
        "id": "d-UKm8V5SEvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xticks(rotation=90)\n",
        "plt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\n",
        "plt.title(\"Frequency of Each Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gp3OuxEhRXCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing Class Frequencies"
      ],
      "metadata": {
        "id": "CMJ_ysvySMjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_class_freqs(labels):\n",
        "    \n",
        "    # total number of patients (rows)\n",
        "    N = labels.shape[0]\n",
        "    \n",
        "    positive_frequencies = np.sum(labels == 1,axis=0)/N\n",
        "    negative_frequencies = np.sum(labels == 0,axis=0)/N\n",
        "\n",
        "    return positive_frequencies, negative_frequencies"
      ],
      "metadata": {
        "id": "32uRbAgNSI38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "labels_matrix = np.array(\n",
        "    [[1, 0, 0],\n",
        "     [0, 1, 1],\n",
        "     [1, 0, 1],\n",
        "     [1, 1, 1],\n",
        "     [1, 0, 1]]\n",
        ")\n",
        "print(f\"labels: {labels_matrix}\")\n",
        "\n",
        "test_pos_freqs, test_neg_freqs = compute_class_freqs(labels_matrix)\n",
        "\n",
        "print(f\"pos freqs: {test_pos_freqs}\")\n",
        "\n",
        "print(f\"neg freqs: {test_neg_freqs}\")"
      ],
      "metadata": {
        "id": "XsPUAuA7SI7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_pos, freq_neg = compute_class_freqs(train_generator.labels)"
      ],
      "metadata": {
        "id": "YZYFT7CwSI-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
      ],
      "metadata": {
        "id": "q089iLx-SJAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_weights = freq_neg\n",
        "neg_weights = freq_pos\n",
        "pos_contribution = freq_pos * pos_weights \n",
        "neg_contribution = freq_neg * neg_weights"
      ],
      "metadata": {
        "id": "4yj_lkLQSJDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n",
        "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n",
        "                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
      ],
      "metadata": {
        "id": "Q4mweiL3SJGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weighted Loss"
      ],
      "metadata": {
        "id": "sMdw3JIISrRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
        "\n",
        "    def weighted_loss(y_true, y_pred):\n",
        "\n",
        "        # initialize loss to zero\n",
        "        loss = 0.0\n",
        "\n",
        "        for i in range(len(pos_weights)):\n",
        "            # for each class, add average weighted loss for that class \n",
        "            \n",
        "            loss += (-1*K.mean(neg_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i]+epsilon))\n",
        "                  +(-1*K.mean(pos_weights[i] * y_true[:, i] * K.log(y_pred[:, i]+epsilon)))) #complete this line\n",
        "        return loss\n",
        "    \n",
        "    return weighted_loss"
      ],
      "metadata": {
        "id": "ShpqVw0VSsSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "sess = K.get_session()\n",
        "with sess.as_default() as sess:\n",
        "    print(\"Test example:\\n\")\n",
        "    y_true = K.constant(np.array(\n",
        "        [[1, 1, 1],\n",
        "         [1, 1, 0],\n",
        "         [0, 1, 0],\n",
        "         [1, 0, 1]]\n",
        "    ))\n",
        "    print(\"y_true:\\n\")\n",
        "    print(y_true.eval())\n",
        "\n",
        "    w_p = np.array([0.25, 0.25, 0.5])\n",
        "    w_n = np.array([0.75, 0.75, 0.5])\n",
        "    print(\"\\nw_p:\\n\")\n",
        "    print(w_p)\n",
        "\n",
        "    print(\"\\nw_n:\\n\")\n",
        "    print(w_n)\n",
        "\n",
        "    y_pred_1 = K.constant(0.7*np.ones(y_true.shape))\n",
        "    print(\"\\ny_pred_1:\\n\")\n",
        "    print(y_pred_1.eval())\n",
        "\n",
        "    y_pred_2 = K.constant(0.3*np.ones(y_true.shape))\n",
        "    print(\"\\ny_pred_2:\\n\")\n",
        "    print(y_pred_2.eval())\n",
        "\n",
        "    # test with a large epsilon in order to catch errors\n",
        "    L = get_weighted_loss(w_p, w_n, epsilon=1)\n",
        "\n",
        "    print(\"\\nIf we weighted them correctly, we expect the two losses to be the same.\")\n",
        "    L1 = L(y_true, y_pred_1).eval()\n",
        "    L2 = L(y_true, y_pred_2).eval()\n",
        "    print(f\"\\nL(y_pred_1)= {L1:.4f}, L(y_pred_2)= {L2:.4f}\")\n",
        "    print(f\"Difference is L1 - L2 = {L1 - L2:.4f}\")"
      ],
      "metadata": {
        "id": "J7QUgw1qSsVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet121"
      ],
      "metadata": {
        "id": "db7hKc_PS-P9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the base pre-trained model\n",
        "base_model = DenseNet121(weights='./nih/densenet.hdf5', include_top=False)\n",
        "\n",
        "x = base_model.output\n",
        "\n",
        "# add a global spatial average pooling layer\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# and a logistic layer\n",
        "predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights))"
      ],
      "metadata": {
        "id": "eB9RerG1SsYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "3TMa2CSUTBYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                              validation_data=valid_generator,\n",
        "                              steps_per_epoch=100, \n",
        "                              validation_steps=25, \n",
        "                              epochs = 3)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zx79wLkMSsbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training on the Larger Dataset"
      ],
      "metadata": {
        "id": "H0DlU6SOTKsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"./nih/pretrained_model.h5\")"
      ],
      "metadata": {
        "id": "y-T9a-bPRXEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction and Evaluation"
      ],
      "metadata": {
        "id": "RKmBy4vvTP4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))"
      ],
      "metadata": {
        "id": "4U3cYhjGTNpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC Curve and AUROC"
      ],
      "metadata": {
        "id": "93_qiJOFTTDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator)"
      ],
      "metadata": {
        "id": "px-uA7LjTNsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Learning with GradCAM"
      ],
      "metadata": {
        "id": "ZHrkgwraTbV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"nih/train-small.csv\")\n",
        "IMAGE_DIR = \"nih/images-small/\"\n",
        "\n",
        "# only show the lables with top 4 AUC\n",
        "labels_to_show = np.take(labels, np.argsort(auc_rocs)[::-1])[:4]"
      ],
      "metadata": {
        "id": "FiOjREdFTNvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00008270_015.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "QEhjz0HRTNzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00011355_002.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "QiDdtn3vQ5_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00029855_001.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "u0NZFvs4Tkav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "util.compute_gradcam(model, '00005410_000.png', IMAGE_DIR, df, labels, labels_to_show)"
      ],
      "metadata": {
        "id": "kGCLxj0MTkdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finish"
      ],
      "metadata": {
        "id": "LnHojSZCTrnL"
      }
    }
  ]
}